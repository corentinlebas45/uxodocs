# üß† Core Concepts

This section explains the primary entities and concepts that form the **uxopian-ai** framework. Understanding these concepts is essential for effective configuration and interaction.

---

## üîå Providers

A **Provider** is a connector to an external Large Language Model (LLM) service. The framework uses providers to abstract the specific implementation details of each LLM service, offering a unified interface.

**Examples:** `openai`, `azure`, `anthropic`, `mistral`

**Configuration:**
The active providers and their API keys are configured in your `.yml` files. This sets the global default.

**Extensibility:**
You can add custom providers by implementing the `ModelProvider` Java interface.

---

## üß† Models

A **Model** refers to a specific language model available through a provider. Each provider supports one or more models with different capabilities and costs.

**Examples:**
`gpt-4o` (from OpenAI), `claude-3-5-sonnet-20240620` (from Anthropic)

**Configuration:**
You can set a default model for the entire framework. This default can be overridden at the prompt level or during an API call.

---

## ‚öñÔ∏è Parameter Precedence (Provider, Model, Temperature & Reasoning)

When making a call to an LLM, you can specify the provider, model, temperature, and reasoning at multiple levels. The framework uses a clear order of precedence to determine which values to use:

1. **API Call Parameters:**
   Values for provider, model, temperature, or reasoning passed directly in the request to the LLM endpoint will always be used first. This offers maximum flexibility for a single call.

2. **Prompt-Specific Defaults:**
   If a parameter is not specified in the API call, the framework will look for default values (`defaultLlmProvider`, `defaultLlmModel`) defined within the prompt itself.

3. **Global Defaults:**
   If no specific parameters are found in either the API call or the prompt, the framework will fall back to the global default values defined in your `.yml` configuration files.

---

## üí¨ Conversations

A **Conversation** is a container that groups a sequence of exchanges between a user and the AI. It maintains the context and history of the interaction.

**Persistence:**
Conversations and their associated messages are stored in OpenSearch.

**Context:**
The framework automatically retrieves recent messages from the current conversation to provide context for new requests, enabling stateful interactions.

**Key Attributes:**

- `title`: The title of the conversation.
- `updatedAt`: Timestamp of the last interaction.
- `llmProvider`: The last LLM provider used in the conversation.
- `llmModel`: The last LLM model used.

---

## ‚úâÔ∏è Messages

A **Message** represents a single turn in a conversation. It can be a user's query or the AI's response. When sending a message, you can provide one of the following (processed in order of priority):

- `goalName`: To execute a high-level task.
- `promptId`: To use a specific, pre-defined prompt.
- `content`: A simple, direct text query.

**Persistence:**
All messages are stored in OpenSearch, linked to their parent conversation.

**Key Attributes:**

- `content`: The initial content/question of the message.
- `answer`: The response generated by the LLM.
- `promptRole`: The role of the message sender (user, assistant, or system).
- `createdAt`: Timestamp of when the message was created.
- `inputTokenCount` & `outputTokenCount`: The number of tokens used, for cost tracking.
- `llmName`: The name of the language model that processed the message.
- `feedback`: An optional ID linking to user feedback on the response.

---

## üìú Prompts

A **Prompt** is a reusable, templated instruction sent to a model to guide its response. Prompts are the core building blocks for interacting with LLMs.

**Templating:**
They use the Thymeleaf engine, allowing for dynamic content using variables (e.g., `${payload.documentId}`) and advanced logic.

**Storage:**
Prompts are stored and managed in OpenSearch. They can be created and updated via the REST API.

**Key Attributes:**

- `role`: Role of the prompt sender (e.g., USER, ASSISTANT, SYSTEM).
- `content`: Content of the prompt.
- `defaultLlmProvider`: The default LLM provider for the prompt (Used if no provider is specified in the API call).
- `defaultLlmModel`: The default LLM model for the prompt (Used if no model is specified in the API call).
- `temperature`: The default temperature for the prompt (Used if no temperature is specified in the API call).
- `reasoningEnabled`: Whether reasoning is enabled for the prompt (Used if no reasoning setting is specified in the API call).

---

## üéØ Goals

A **Goal** is a high-level, reusable task that orchestrates which Prompt to use based on a given context. A goal is essentially a mapping between a specific situation (defined by a filter) and a specific prompt.

**Example Use Case:**
A goal named `compare` could use the `detailedComparison` prompt if the document type is a `contract`, but use the `genericComparison` prompt otherwise.

**Filtering:**
The filter logic uses Spring Expression Language (SpEL) to evaluate the context sent in the API call's payload.

**Storage:**
Like prompts, goals are stored and managed in OpenSearch.
